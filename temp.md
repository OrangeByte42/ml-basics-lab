神经网络训练过程：
1. 定义简单的神经网络架构；
2. 数据处理；
3. 指定损失函数；
4. 如何训练模型；

数据流水线、模型、损失函数、小批量随机梯度下降优化器

步骤：
1. 生成/加载数据到内存；
2. 将数据按 batch 组织，并构建训练数据、测试数据对应的数据迭代器；
    - 相比于直接迭代需要将数据全部加载到内存中并执行大量随机内存访问，数据迭代器执行效率要高得多；
    - 数据迭代器可处理在文件中的数据和数据流提供的数据；
3. 构建模型；
    - 设计构建模型架构、其包含的参数集合，包括前向传播、反向传播过程；
    - 初始化模型参数，之后的训练任务即是更新这些参数，直至足够拟合数据；



正如我们在 3.1节中讨论的,线性回归有解析解。尽管线性回归有解析解,但本书中的其他模型却没有。这里  我们介绍小批量随机梯度下降。

在每一步中,使用从数据集中随机抽取的一个小批量,然后根据参数计算损失的梯度。接下来,朝着减少损失  的方向更新我们的参数。下面的函数实现小批量随机梯度下降更新。该函数接受模型参数集合、学习速率和  批量大小作为输入。每一步更新的大小由学习速率lr决定。因为我们计算的损失是一个批量样本的总和,所  以我们用批量大小(batch_size)来规范化步⻓,这样步⻓大小就不会取决于我们对批量大小的选择。

现在我们已经准备好了模型训练所有需要的要素,可以实现主要的训练过程部分了。理解这段代码至关重要,  因为从事深度学习后,相同的训练过程几乎一遍又一遍地出现。在每次迭代中,我们读取一小批量训练样本,  并通过我们的模型来获得一组预测。计算完损失后,我们开始反向传播,存储每个参数的梯度。最后,我们  调用优化算法sgd来更新模型参数。

概括一下,我们将执行以下循环:
• 初始化参数
• 重复以下训练,直到完成
    – 计算梯度g @(w;b) 1  jBj  ∑  i2B l(x(i); y(i); w; b)
    – 更新参数(w; b) (w; b) g

在每个迭代周期(epoch)中,我们使用data_iter函数遍历整个数据集,并将训练数据集中所有样本都使用  一次(假设样本数能够被批量大小整除)。这里的迭代周期个数num_epochs和学习率lr都是超参数,分别设  为3和0.03。设置超参数很棘手,需要通过反复试验进行调整。我们现在忽略这些细节,以后会在 11节中详细  介绍。

注意,我们不应该想当然地认为我们能够完美地求解参数。在机器学习中,我们通常不太关心恢复真正的参  数,而更关心如何高度准确预测参数。幸运的是,即使是在复杂的优化问题上,随机梯度下降通常也能找到  非常好的解。其中一个原因是,在深度网络中存在许多参数组合能够实现高度精确的预测。


# 问题

线性回归就是逻辑回归吗？https://chatgpt.com/share/6858e776-11fc-8005-8676-9036f8c7450f



# 深度学习框架

当我们在 3.2节中实现线性回归时,我们明确定义了模型参数变量,并编写了计算的代码,这样通过基本的  线性代数运算得到输出。但是,如果模型变得更加复杂,且当我们几乎每天都需要实现模型时,自然会想简  化这个过程。

对于标准深度学习模型,我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型,而  不必关注层的实现细节。我们首先定义一个模型变量net,它是一个Sequential类的实例。Sequential类将多  个层串联在一起。当给定输入数据时,Sequential实例将数据传入到第一层,然后将第一层的输出作为第二  层的输入,以此类推。在下面的例子中,我们的模型只包含一个层,因此实际上不需要Sequential。但是由  于以后几乎所有的模型都是多层的,在这里使用Sequential会让你熟悉“标准的流水线”。

回顾 图3.1.2中的单层网络架构,这一单层被称为全连接层(fully-connected layer),因为它的每一个输入都  通过矩阵-向量乘法得到它的每个输出。

在PyTorch中,全连接层在Linear类中定义。值得注意的是,我们将两个参数传递到nn.Linear中。第一个指  定输入特征形状,即2,第二个指定输出特征形状,输出特征形状为单个标量,因此为1。

在使用net之前,我们需要初始化模型参数。如在线性回归模型中的权重和偏置。深度学习框架通常有预定  义的方法来初始化参数。在这里,我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采  样,偏置参数将初始化为零。

正如我们在构造nn.Linear时指定输入和输出尺寸一样,现在我们能直接访问参数以设定它们的初始值。我  们通过net[0]选择网络中的第一个图层,然后使用weight.data和bias.data方法访问参数。我们还可以使用  替换方法normal_和fill_来重写参数值。

计算均方误差使用的是MSELoss类,也称为平方L2范数。默认情况下,它返回所有样本损失的平均值。

小批量随机梯度下降算法是一种优化神经网络的标准工具,PyTorch在optim模块中实现了该算法的许多变  种。当我们实例化一个SGD实例时,我们要指定优化的参数(可通过net.parameters()从我们的模型中获得)  以及优化算法所需的超参数字典。小批量随机梯度下降只需要设置lr值,这里设置为0.03。

通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。我们不必单独分配参数、不必定义  我们的损失函数,也不必手动实现小批量随机梯度下降。当我们需要更复杂的模型时,高级API的优势将大  大增加。当我们有了所有的基本组件,训练过程代码与我们从零开始实现时所做的非常相似。

回顾一下:在每个迭代周期里,我们将完整遍历一次数据集(train_data),不停地从中获取一个小批量的输  入和相应的标签。对于每一个小批量,我们会进行以下步骤:
• 通过调用net(X)生成预测并计算损失l(前向传播)。
• 通过进行反向传播来计算梯度。
• 通过调用优化器来更新模型参数。

为了更好的衡量训练效果,我们计算每个迭代周期后的损失,并打印它来监控训练过程。

在这里,我们使用学习率为0.1的小批量随机梯度下降作为优化算法。这与我们在线性回归例子中的相同,这  说明了优化器的普适性。


## 全连接层 参数开销

正如我们将在后续章节中看到的,在深度学习中,全连接层无处不在。然而,顾名思义,全连接层是“完全”连  接的,可能有很多可学习的参数。具体来说,对于任何具有 $d$ 个输入和 $q$ 个输出的全连接层,参数开销为 $\mathcal{O}(dq)$,  这个数字在实践中可能高得令人望而却步。幸运的是,将 $d$ 个输入转换为 $q$ 个输出的成本可以减少到 $\mathcal{O(\frac{dq}{n})}$,其  中超参数 $n$ 可以由我们灵活指定,以在实际应用中平衡参数节约和模型有效性 (Zhang et al., 2021)。


# T

现在我们将优化参数以最大化观测数据的概率。为了得到预测结果,我们将设置一个阈值,如选择具有最大  概率的标签。

**LSE 技巧**：
- https://zh.wikipedia.org/wiki/LogSumExp
- https://blog.csdn.net/yjw123456/article/details/121869249




给定预测概率分布y_hat,当我们必须输出硬预测(hard prediction)时,我们通常选择预测概率最高的类。  许多应用都要求我们做出选择。

当预测与标签分类y一致时,即是正确的。分类精度即正确预测数量与总预测数量之比。虽然直接优化精度可  能很困难(因为精度的计算不可导),但精度通常是我们最关心的性能衡量标准,我们在训练分类器时几乎总  会关注它。

为了计算精度,我们执行以下操作。首先,如果y_hat是矩阵,那么假定第二个维度存储每个类的预测分数。  我们使用argmax获得每行中最大元素的索引来获得预测类别。然后我们将预测类别与真实y元素进行比较。由  于等式运算符“==”对数据类型很敏感,因此我们将y_hat的数据类型转换为与y的数据类型一致。结果是一  个包含0(错)和1(对)的张量。最后,我们求和会得到正确预测的数量。




