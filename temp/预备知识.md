# 概述

所有机器学习方法都涉及从数据中提取信息。因此,我们  先学习一些关于数据的实用技能,包括存储、操作和预处理数据。

机器学习通常需要处理大型数据集。我们可以将某些数据集视为一个表,其中表的行对应样本,列对应属性。

为了能够完成各种数据操作,我们需要某种方法来存储和操作数据。通常,我们需要做两件重要的事:(1)  获取数据;(2)将数据读入计算机后对其进行处理。如果没有某种方法来存储数据,那么获取数据是没有意  义的。

我们想在这些数据上执行数学运算,其中最简单且最有用的操作  是按元素(elementwise)运算。它们将标准标量运算符应用于数组的每个元素。对于将两个数组作为输入  的函数,按元素运算将二元运算符应用于两个数组中的每对位置对应的元素。我们可以基于任何从标量到标  量的函数来创建按元素函数。

在某些情况下,即使形状不同,  我们仍然可以通过调用 广播机制(broadcasting mechanism)来执行按元素操作。这种机制的工作方式如  下:

1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对生成的数组执行按元素操作；

在大多数情况下，我们沿着数组中长度为 1 的轴进行广播；

就像在任何其他Python数组中一样,张量中的元素可以通过索引访问。与任何Python数组一样:第一个元素  的索引是0,最后一个元素索引是-1;可以指定范围以包含第一个元素和最后一个之前的元素。

# 使用 PyTorch

导入 `torch`：

```python
import torch
```

# 张量

- 无论使用哪个深度学习框架，它的张量类（在 MXNet 中为 ndarray，在 PyTorch 和 TensorFlow 中为 Tensor）都与 NumPy 的 ndarray 类似。但深度学习框架比 NumPy 的 ndarray 多一些重要功能，这些功能使得张量类更适合深度学习：
    - GPU 很好地支持加速计算，而 NumPy 仅支持 CPU 计算；
    - 张量类支持自动微分；
- $n$ 维数组，也称为张量。张量表示一个由数值组成的数组，这个数组可能有多个维度；
    - 具有一个轴的张量对应数学上的 向量（vector）；
    - 具有两个轴的张量对应数学上的 矩阵（matrix）；
    - 具有两个轴以上的张量没有特殊的数学名称；
- 张量中的每个值都称为张量的元素（element）；
- 除非额外指定，新的张量将存储在内存中，并采用基于 CPU 的计算；

### 节省内存

- 运行一些操作可能会导致为新结果分配内存。例如,如果我们用Y = X + Y,我们将取消引用Y指向的张量,  而是指向新分配的内存处的张量。
- 这可能是不可取的,原因有两个:
    1. 首先,我们不想总是不必要地分配内存。在机器学习中,我们可能有数百兆的参数,并且在一秒内多次  更新所有参数。通常情况下,我们希望原地执行这些更新;
    2. 如果我们不原地更新,其他引用仍然会指向旧的内存位置,这样我们的某些代码可能会无意中引用旧  的参数。
- 两种解决方法：
    1. 幸运的是,执行原地操作非常简单。我们可以使用切片表示法将操作的结果分配给先前分配的数组,例如`Y[:]  = <expression>`；
    2. 如果在后续计算中没有重复使用X,我们也可以使用X[:] = X + Y或X += Y来减少操作的内存开销；

将深度学习框架定义的张量转换为NumPy张量(ndarray)很容易,反之也同样容易。torch张量和numpy数  组将共享它们的底层内存,就地操作更改一个张量也会同时更改另一个张量。

要将大小为1的张量转换为Python标量,我们可以调用item函数或Python的内置函数。

深度学习存储和操作数据的主要接口是张量(n维数组)。它提供了各种功能,包括基本数学运算、广  播、索引、切片、内存节省和转换其他Python对象。

# 数据预处理

为了能用深度学习来解决现实世界的问题,我们经常从预处理原始数据开始,而不是从那些准备好的张量格  式数据开始。在Python中常用的数据分析工具中,我们通常使用pandas软件包。像庞大的Python生态系统中  的许多其他扩展包一样,pandas可以与张量兼容。

注意,“NaN”项代表缺失值。为了处理缺失的数据,典型的方法包括插值法和删除法：

- 插值法：用一个替代值弥补缺失值；
- 删除法：直接忽略缺失值；

pandas软件包是Python中常用的数据分析工具中,pandas可以与张量兼容。

用pandas处理缺失的数据时,我们可根据情况选择用插值法和删除法。



当向  量表示数据集中的样本时,它们的值具有一定的现实意义。



# 数学应用相关

矩阵是有用的数据结构:它们允许我们组织具有不同模式的数据。

## 为什么通常以行向量来组织样本？

因此,尽管单个向量的默认方向是列向量,但在表示表格数据集的矩阵中,将每个数据样本作为矩  阵中的行向量更为常⻅。后面的章节将讲到这点,这种约定将支持常⻅的深度学习实践。例如,沿着张量的  最外轴,我们可以访问或遍历小批量的数据样本。

标量、向量、矩阵和任意数量轴的张量(本小节中的“张量”指代数对象)有一些实用的属性。例如,从按  元素操作的定义中可以注意到,任何按元素的一元运算都不会改变其操作数的形状。同样,给定具有相同形  状的任意两个张量,任何按元素二元运算的结果都将是相同形状的张量。例如,将两个相同形状的矩阵相加,  会在这两个矩阵上执行元素加法。

点积在很多场合都很有用。例如，给定一组由向量 $\mathbf{x} \in \mathbb{R}^{d}$ 表示的值，和一组由 $\mathbf{w} \in \mathbb{R}^{d}$ 表示的权重。$\mathbf{x}$ 中的值根据权重 $\mathbf{w}$ 的加权和，可以表示为点积 $\mathbf{x}^{T}\mathbf{w}$。
- 当权重为非负数且和为 $1$（即 $\sum\limits_{i=1}^{d}w_i=1$）时，点积表示加权平均（weighted average）；
- 将两个向量规范化得到单位长度后，点积表示它们夹角的余弦；

我们可以把一个矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 乘法看作一个从 $\mathbb{R}^{n}$ 到 $\mathbb{R}^{m}$ 的转换。这些转换是非常有用的，例如可以用方阵的乘法表示旋转。后续章节将讲到,我们也可以使用矩阵-向量积来描述在给定前一层的值时,求解神经  网络每一层所需的复杂计算。

范数听起来很像距离的度量。欧几里得距离和毕达哥拉斯定理中的非负性概念和三⻆不等式可能会给出一些  启发。


