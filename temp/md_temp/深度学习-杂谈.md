## 背景介绍

机器学习是一⻔具有前瞻性的学科,在现实世界的应用范围很窄。而那些应用,例如语音识别和计算机视觉,需要大量的领域知识,以至于它们通常被认为是完全独立的领域,而机器学习对这些领域来说只是一个小组件。因此,神经网络——我们在本书中关注的深度学习模型的前身,被认为是过时的工具。

深度学习已经彻底改变了模式识别,引入了一系列技术,包括计算机视觉、自然语言处理、自动语音识别。

就在过去的五年里,深度学习给世界带来了惊喜,推动了计算机视觉、自然语言处理、自动语音识别、强化学习和统计建模等领域的快速发展。

测试深度学习的潜力带来了独特的挑战,因为任何一个应用都会将不同的学科结合在一起。应用深度学习需要同时了解(1)以特定方式提出问题的动机;(2)给定建模方法的数学;(3)将模型拟合数据的优化算法;(4)能够有效训练模型、克服数值计算缺陷并最大限度地利用现有硬件的工程方法。

要成功地应用深度学习,必须知道如何抛出一个问题、建模的数学方法、将模型与数据拟合的算法,以及实现所有这些的工程技术。

有时,我们想深入研究模型的细节,这些的细节通常会被深度学习框架的高级抽象隐藏起来。

时至今日,人们常用的计算机程序几乎都是软件开发人员从零编写的。其中,这个应用程序的核心——“业务逻辑”,详细说明了应用程序在各种情况下进行的操作。为了完善业务逻辑,开发人员必须细致地考虑应用程序所有可能遇到的边界情况,并为这些边界情况设计合适的规则。虽然一次编写出完美应用程序的可能性微乎其微,但在大多数情况下,开发人员可以从上述的业务逻辑出发,编写出符合业务逻辑的应用程序,并不断测试直到满足用戶的需求。根据业务逻辑设计自动化系统,驱动正常运行的产品和系统,是一个人类认知上的非凡壮举。

幸运的是,对日益壮大的机器学习科学家群体来说,实现很多任务的自动化并不再屈从于人类所能考虑到的逻辑。有时任务可能遵循一种随着时间推移而变化的模式,我们需要程序来自动调整。有时任务内的关系可能太复杂(比如像素和抽象类别之间的关系),需要数千或数百万次的计算。即使人类的眼睛能毫不费力地完成这些难以提出完美解决方案的任务,这其中的计算也超出了人类意识理解范畴。机器学习(machine learning,ML)是一类强大的可以从经验中学习的技术。通常采用观测数据或与环境交互的形式,机器学习算法会积累更多的经验,其性能也会逐步提高。相反,对于刚刚所说的电子商务平台,如果它一直执行相同的业务逻辑,无论积累多少经验,都不会自动提高,除非开发人员认识到问题并更新软件。

通常,即使我们不知道怎样明确地告诉计算机如何从输入映射到输出,大脑仍然能够自己执行认知功能。换句话说,即使我们不知道如何编写计算机程序来识别“Alexa”这个词,大脑自己也能够识别它。

在开始用机器学习算法解决问题之前,我们必须精确地定义问题,确定输入(input)和输出(output)的性质,并选择合适的模型族。

深度学习是关于优化的学习。对于一个带有参数的模型,我们想要找到其中能拟合数据的最好模型。

机器学习还涉及如何做出预测:给定观察到的信息,某些未知属性可能的值是多少?要在不确定的情况下进  行严格的推断,我们需要借用概率语言。

在深度学习中,我们“训练”模型,不断更新它们,使它们在看到越来越多的数据时变得越来越好。通常情  况下,变得更好意味着最小化一个损失函数(loss function),即一个衡量“模型有多糟糕”这个问题的分数。  最终,我们真正关心的是生成一个模型,它能够在从未⻅过的数据上表现良好。但“训练”模型只能将模型  与我们实际能看到的数据相拟合。因此,我们可以将拟合模型的任务分解为两个关键问题:
- 优化（optimization）：用模型拟合观测数据的过程；
- 泛化（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型；


### 自动微分

求导是几乎所有深度学习优化算法的关键步骤。虽然求导的计算很简单,只需要一些基  本的微积分。但对于复杂的模型,手工进行更新是一件很痛苦的事情(而且经常容易出错)。

深度学习框架通过自动计算导数,即自动微分(automatic differentiation)来加快求导。实际中,根据设计  好的模型,系统会构建一个计算图(computational graph),来跟踪计算是哪些数据通过哪些操作组合起来  产生输出。自动微分使系统能够随后反向传播梯度。这里,反向传播(backpropagate)意味着跟踪整个计算  图,填充关于每个参数的偏导数。

**标量变量的反向传播**：在我们计算y关于x的梯度之前,需要一个地方来存储梯度。重要的是,我们不会在每次对一个参数求导时都  分配新的内存。因为我们经常会成千上万次地更新相同的参数,每次都分配新的内存可能很快就会将内存耗  尽。注意,一个标量函数关于向量x的梯度是向量,并且与x具有相同的形状。

**非标量变量的反向传播**：
- 当y不是标量时,向量y关于向量x的导数的最自然解释是一个矩阵。对于高阶和高维的y和x,求导的结果可以  是一个高阶张量。
- 然而,虽然这些更奇特的对象确实出现在高级机器学习中(包括深度学习中),但当调用向量的反向计算时,  我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。这里,我们的目的不是计算微分矩阵,  而是单独计算批量中每个样本的偏导数之和。


## 历史问题

为了解决各种各样的机器学习问题,深度学习提供了强大的工具。虽然许多深度学习方法都是最近才有重大  突破,但使用数据和神经网络编程的核心思想已经研究了几个世纪。事实上,人类⻓期以来就有分析数据和  预测未来结果的愿望,而自然科学大部分都植根于此。

神经网络（neural networks）的得名源于生物灵感；

其核心是当今大多数网络中都可以找到的几个关键原则：
- 线性和非线性处理单元的交替，通常称为 层（layers）；
- 使用链式规则（也称为 反向传播（backpropagation））一次性调整网络中的全部参数；

考虑到数据和计算的稀缺性,核方法  (kernel method)、决策树(decision tree)和图模型(graph models)等强大的统计工具(在经验上)证明 是更为优越的。与神经网络不同的是,这些算法不需要数周的训练,而且有很强的理论依据,可以提供可预  测的结果。

大约 2010 年开始，那些在计算上看起来不可行的神经网络算法变得热门起来，实际上是以下两点导致的：

1. 互联网公司出现，为数亿在线用户提供服务，大规模数据集变得触手可及；
2. 廉价高质量的传感器、数据存储（克莱德定律）以及廉价计算（摩尔定律）的普及，特别是 GPU 的普及，使得大规模算力唾手可得；

很明显,随机存取存储器没有跟上数据增⻓的步伐。与此同时,算力的增⻓速度已经超过了现有数据的增⻓  速度。这意味着统计模型需要提高内存效率(这通常是通过添加非线性来实现的),同时由于计算预算的增  加,能够花费更多时间来优化这些参数。因此,机器学习和统计的关注点从(广义的)线性模型和核方法转  移到了深度神经网络。

许多深度学习中的中流砥柱，如：多层感知机；卷积神经网络；长短期记忆网络；Q学习；

过去十年中的巨大进步：

- 新的容量控制方法：如 dropout，有助于减轻过拟合的危险，这是通过在整个神经网络中应用噪声注入来实现的，处于训练目的，用随机变量来代替权重；
- 注意力机制解决了困扰统计学一个多世纪的问题：如何在不增加可学习参数的情况下增加系统的记忆和复杂性；
- 多阶段设计：如，存储器网络 和 神经编程器-解释器；
- 生成对抗网络；
- 构建并行和分布式训练算法的能力显著提升，多 GPU 用于训练大量数据；
- 并行计算能力对强化学习的进步做出了关键贡献；
- 深度学习框架在传播思想方面发挥至关重要的作用；
    - 第一代框架：Caffe、Torch、Theano；
    - 第二代工具：TensorFlow（通常通过其高级 API Keras 使用）、CNTK、Caffe 2、Apache MXNet；
    - 第三代工具：用于深度学习的命令式工具，由 Chainer 率先推出，使用类似于 Python NumPy 的语法来描述模型。PyTorch、MXNet 的 Gluon API 和 Jax；

”系统研究人员构建更好的工具“ 和 ”统计建模人员构建更好的神经网络“ 之间的分工大大简化了工作。

机器学习是无处不在的，尽管它经常隐藏在视线之外；

机器学习应用场景：智能助理、语音识别、物体识别、对抗游戏、自动驾驶、

关于人工智能的非技术性文章中,经常提到人工智能奇点的问题:机器学习系统会变得有知觉,并独立于主  人来决定那些直接影响人类生计的事情。在某种程度上,人工智能已经直接影响到人类的生计:信誉度的自  动评估,⻋辆的自动驾驶,保释决定的自动准予等等。甚至,我们可以让Alexa打开咖啡机。

幸运的是,我们离一个能够控制人类创造者的有知觉的人工智能系统还很远。首先,人工智能系统是以一种  特定的、面向目标的方式设计、训练和部署的。虽然他们的行为可能会给人一种通用智能的错觉,但设计的  基础是规则、启发式和统计模型的结合。其次,目前还不存在能够自我改进、自我推理、能够在试图解决一  般任务的同时,修改、扩展和改进自己的架构的“人工通用智能”工具。

如果不加注  意地应用统计模型,可能会导致种族、性别或年龄偏⻅,如果自动驱动相应的决策,则会引起对程序公平性  的合理关注。重要的是要确保小心使用这些算法。就我们今天所知,这比恶意超级智能毁灭人类的⻛险更令  人担忧。

机器学习,它既是人工智能的一个分支,也是人工智能的一种方法。虽  然深度学习是机器学习的一个子集,但令人眼花缭乱的算法和应用程序集让人很难评估深度学习的具体成分  是什么。

如前所述,机器学习可以使用数据来学习输入和输出之间的转换

深度学习是“深度”的,模型  学习了许多“层”的转换,每一层提供一个层次的表示。例如,靠近输入的层可以表示数据的低级细节,而  接近分类输出的层可以表示用于区分的更抽象的概念。由于表示学习(representation learning)目的是寻  找表示本身,因此深度学习可以称为“多级表示学习”。

事实证明,这些多层模型能够以以  前的工具所不能的方式处理低级的感知数据。毋庸置疑,深度学习方法中最显著的共同点是使用端到端训练。  也就是说,与其基于单独调整的组件组装系统,不如构建系统,然后联合调整它们的性能。

- 例如,在计算机视  觉中,科学家们习惯于将特征工程的过程与建立机器学习模型的过程分开。
- Canny边缘检测器 (Canny, 1987)  和SIFT特征提取器 (Lowe, 2004) 作为将图像映射到特征向量的算法,在过去的十年里占据了至高无上的地  位。

在过去的日子里,将机器学习应用于这些问题的关键部分是提出人工设计的特征工程方法,将数据转换  为某种适合于浅层模型的形式。然而,与一个算法自动执行的数百万个选择相比,人类通过特征工程所能完  成的事情很少。当深度学习开始时,这些特征抽取器被自动调整的滤波器所取代,产生了更高的精确度。

因此,深度学习的一个关键优势是它不仅取代了传统学习管道末端的浅层模型,而且还取代了劳动密集型的  特征工程过程。此外,通过取代大部分特定领域的预处理,深度学习消除了以前分隔计算机视觉、语音识别、  自然语言处理、医学信息学和其他应用领域的许多界限,为解决各种问题提供了一套统一的工具。

除了端到端的训练,人们正在经历从参数统计描述到完全非参数模型的转变。当数据稀缺时,人们需要依靠  简化对现实的假设来获得有用的模型。当数据丰富时,可以用更准确地拟合实际情况的非参数模型来代替。  在某种程度上,这反映了物理学在上个世纪中叶随着计算机的出现所经历的进步。现在人们可以借助于相关  偏微分方程的数值模拟,而不是用手来求解电子行为的参数近似。这导致了更精确的模型,尽管常常以牺牲  可解释性为代价。

与以前工作的另一个不同之处是接受次优解,处理非凸非线性优化问题,并且愿意在证明之前尝试。这种在  处理统计问题上新发现的经验主义,加上人才的迅速涌入,导致了实用算法的快速进步。尽管在许多情况下,  这是以修改和重新发明存在了数十年的工具为代价的。

机器学习研究计算机系统如何利用经验(通常是数据)来提高特定任务的性能。它结合了统计学、数据  挖掘和优化的思想。通常,它是被用作实现人工智能解决方案的一种手段。

表示学习作为机器学习的一类,其研究的重点是如何自动找到合适的数据表示方式。深度学习是通过  学习多层次的转换来进行的多层次的表示学习。

深度学习不仅取代了传统机器学习的浅层模型,而且取代了劳动密集型的特征工程。

最近在深度学习方面取得的许多进展,大都是由廉价传感器和互联网规模应用所产生的大量数据,以  及(通过GPU)算力的突破来触发的。

整个系统优化是获得高性能的关键环节。有效的深度学习框架的开源使得这一点的设计和实现变得非  常容易。

线性回归发明的时间(1795年)早于计算神经科学,所以将线性回归描述为神经网络似乎不合适。当控制学  家、神经生物学家沃伦·⻨库洛奇和沃尔特·皮茨开始开发人工神经元模型时,他们为什么将线性模型作为一  个起点呢?我们来看一张图片 图3.1.3:这是一张由树突(dendrites,输入终端)、细胞核(nucleus,CPU)  组成的生物神经元图片。轴突(axon,输出线)和轴突端子(axon terminal,输出端子)通过突触(synapse)  与其他神经元连接。树突中接收到来自其他神经元(或视网膜等环境传感器)的信息xi。该信息通过突触权重wi来加权,以确定  输入的影响(即,通过xiwi相乘来激活或抑制)。来自多个源的加权输入以加权和y = ∑  i xiwi + b的形式汇聚 在细胞核中,然后将这些信息发送到轴突y中进一步处理,通常会通过 (y)进行一些非线性处理。之后,它要  么到达目的地(例如肌肉),要么通过树突进入另一个神经元。  当然,许多这样的单元可以通过正确连接和正确的学习算法拼凑在一起,从而产生的行为会比单独一个神经  元所产生的行为更有趣、更复杂,这种想法归功于我们对真实生物神经系统的研究。  当今大多数深度学习的研究几乎没有直接从神经科学中获得灵感。我们援引斯图尔特·罗素和彼得·诺维格在  他们的经典人工智能教科书 Artificial Intelligence:A Modern Approach (Russell and Norvig, 2016) 中所说的: 虽然⻜机可能受到⻦类的启发,但几个世纪以来,⻦类学并不是航空创新的主要驱动力。同样地,如今在深  度学习中的灵感同样或更多地来自数学、统计学和计算机科学。



# 小结

~ 线性代数：
• 标量、向量、矩阵和张量是线性代数中的基本数学对象。
• 向量泛化自标量,矩阵泛化自向量。
• 标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。
• 一个张量可以通过sum和mean沿指定的轴降低维度。
• 两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。
• 在深度学习中,我们经常使用范数,如L1范数、L2范数和Frobenius范数。
• 我们可以对标量、向量、矩阵和张量执行各种操作。

~ 微积分：
• 微分和积分是微积分的两个分支,前者可以应用于深度学习中的优化问题。
• 导数可以被解释为函数相对于其变量的瞬时变化率,它也是函数曲线的切线的斜率。
• 梯度是一个向量,其分量是多变量函数相对于其所有变量的偏导数。
• 链式法则可以用来微分复合函数。

~ 自动求导：
• 深度学习框架可以自动计算导数:我们首先将梯度附加到想要对其计算偏导数的变量上,然后记录目  标值的计算,执行它的反向传播函数,并访问得到的梯度。

~ 概率：
• 我们可以从概率分布中采样。
• 我们可以使用联合分布、条件分布、Bayes定理、边缘化和独立性假设来分析多个随机变量。
• 期望和方差为概率分布的关键特征的概括提供了实用的度量形式。

~ 线性回归：
• 机器学习模型中的关键要素是训练数据、损失函数、优化算法,还有模型本身。
• 矢量化使数学表达上更简洁,同时运行的更快。
• 最小化目标函数和执行极大似然估计等价。
• 线性回归模型也是一个简单的神经网络。

• 我们学习了深度网络是如何实现和优化的。在这一过程中只使用张量和自动微分,不需要定义层或复  杂的优化器。
• 这一节只触及到了表面知识。在下面的部分中,我们将基于刚刚介绍的概念描述其他模型,并学习如何  更简洁地实现其他模型。

• 我们可以使用PyTorch的高级API更简洁地实现模型。
• 在PyTorch中,data模块提供了数据处理工具,nn模块定义了大量的神经网络层和常⻅损失函数。
• 我们可以通过_结尾的方法将参数替换,从而初始化参数。

~ Softmax 回归：
• softmax运算获取一个向量并将其映射为概率。
• softmax回归适用于分类问题,它使用了softmax运算中输出类别的概率分布。
• 交叉熵是一个衡量两个概率分布之间差异的很好的度量,它测量给定模型编码数据所需的比特数。

• Fashion-MNIST是一个服装分类数据集,由10个类别的图像组成。我们将在后续章节中使用此数据集来  评估各种分类算法。
• 我们将高度h像素,宽度w像素图像的形状记为h w或(h,w)。
• 数据迭代器是获得更高性能的关键组件。依靠实现良好的数据迭代器,利用高性能计算来避免减慢训练过程。

• 借助softmax回归,我们可以训练多分类的模型。
• 训练softmax回归循环模型与训练线性回归模型非常相似:先读取数据,再定义模型和损失函数,然后  使用优化算法训练模型。大多数常⻅的深度学习模型都有类似的训练过程。

• 使用深度学习框架的高级API,我们可以更简洁地实现softmax回归。
• 从计算的⻆度来看,实现softmax回归比较复杂。在许多情况下,深度学习框架在这些著名的技巧之外  采取了额外的预防措施,来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇  到的陷阱。

• 使用深度学习框架的高级API,我们可以更简洁地实现softmax回归。
• 从计算的⻆度来看,实现softmax回归比较复杂。在许多情况下,深度学习框架在这些著名的技巧之外  采取了额外的预防措施,来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇  到的陷阱。

在这个过程中,我们学习了如何  处理数据,如何将输出转换为有效的概率分布,并应用适当的损失函数,根据模型参数最小化损失。

~ 多层感知机
• 多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层,并通过激活函数转换隐藏层的输出。
• 常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。

• 手动实现一个简单的多层感知机是很容易的。然而如果有大量的层,从零开始实现多层感知机会变得  很麻烦(例如,要命名和记录模型的参数)。

• 我们可以使用高级API更简洁地实现多层感知机。
• 对于相同的分类问题,多层感知机的实现与softmax回归的实现相同,只是多层感知机的实现里增加了  带有激活函数的隐藏层。

~ 过拟合 与 欠拟合
• 欠拟合是指模型无法继续减少训练误差。过拟合是指训练误差远小于验证误差。
• 由于不能基于训练误差来估计泛化误差,因此简单地最小化训练误差并不一定意味着泛化误差的减小。机器学习模型需要注意防止过拟合,即防止泛化误差过大。
• 验证集可以用于模型选择,但不能过于随意地使用它。
• 我们应该选择一个复杂度适当的模型,避免使用数量不足的训练样本。

~ 权重衰减/正则化
• 正则化是处理过拟合的常用方法:在训练集的损失函数中加入惩罚项,以降低学习到的模型的复杂度。
• 保持模型简单的一个特别的选择是使用L2惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。
• 权重衰减功能在深度学习框架的优化器中提供。
• 在同一训练代码实现中,不同的参数集可以有不同的更新行为。


# 附录

- 线性代数：https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html






相关工具：
- GitHub：共享源代码；
- Jupyter Note：混合代码、公式和文本；
- Sphinx：渲染引擎，生成多个输出，为论坛提供讨论；

书籍推荐：
- Goodfellow et al. 2016 《深度学习》；
- ChrisBishop. 2006 优秀教科书；
- 克劳德·香农 《信息论》；艾伦·图灵 《计算与智能》；
- 唐纳德·赫布 《行为的组织》（Hebb and Hebb, 1949）；
- 线性代数相关资源（Kolter, 2008, Petersen et al. 2008, Strang, 1993）
    - [Kolter, 2008] Kolter, Z. (2008). Linear algebra review and reference. Available online: http.
    - [Petersen et al., 2008] Petersen, K. B., Pedersen, M. S., & others. (2008). The matrix cookbook. Technical  University of Denmark, 7(15), 510.
    - [Strang, 1993] Strang, G. (1993). Introduction to linear algebra. Vol. 3. Wellesley-Cambridge Press Wellesley,  MA.


杂乱知识点：
- 伯努利分布；
- 高斯分布；最小均方算法；
- 修剪均值估计；
- 线性判别分析；费舍尔信息矩阵；
- 信息论；计算理论；
- Rosenblatt 感知器学习算法（”赫布学习“）：该算法为当今深度学习的许多随机梯度下降算法奠定了基础：强化期望行为和减少不良行为，从而在神经网络中获得良好的参数设置；
- 蒙特卡洛树抽样；
- 线性代数：
    - 矩阵可以分解为因子，这些分解可以显示真实世界数据集中的低维结构；
    - 机器学习的整个子领域都侧重于使用矩阵分解及其向高阶张量的泛化，来发现数据集中的结构并解决预测问题；

高尔顿板







